{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Question Answering on Aliceâ€™s Adventures in Wonderland"
      ],
      "metadata": {
        "id": "8cuGdidkZij4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import re\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "dUodrh3VCaff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preprocessing"
      ],
      "metadata": {
        "id": "U7i_isSvZfW7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ETEKo0fOCUum"
      },
      "outputs": [],
      "source": [
        "# loading the book\n",
        "url = \"https://www.gutenberg.org/cache/epub/11/pg11.txt\"\n",
        "response = requests.get(url)\n",
        "text = response.text\n",
        "print(text[:1000]) # getting a preview of the first 1000 characters"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# removing the Glutenberg license text\n",
        "start_marker = \"*** START OF THE PROJECT GUTENBERG EBOOK\"\n",
        "end_marker = \"*** END OF THE PROJECT GUTENBERG EBOOK\"\n",
        "\n",
        "start_idx = text.find(start_marker)\n",
        "end_idx = text.find(end_marker)\n",
        "\n",
        "# extracting only the book text\n",
        "book_text = text[start_idx + len(start_marker):end_idx]\n",
        "\n",
        "# preview\n",
        "print(book_text[:500])"
      ],
      "metadata": {
        "id": "Zlx2ekhcCp2x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# basic cleaning of the data\n",
        "book_text = book_text.replace(\"\\r\", \" \") #removing CR characters\n",
        "book_text = re.sub(r\"\\n\\s*\\n\\s*\\n+\", \"\\n\\n\", book_text) # collapsing 3+ blank lines into 1\n",
        "\n",
        "# stripping the leading/trailing spaces\n",
        "book_text = book_text.strip()\n",
        "\n",
        "print(book_text[:500])"
      ],
      "metadata": {
        "id": "fCg3BwTkFbHm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformer Model Architecture & QA Pipeline"
      ],
      "metadata": {
        "id": "kmNq8624Zqd1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading the Pretrained QA Model"
      ],
      "metadata": {
        "id": "KTkvhC4EawRp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\n",
        "\n",
        "# model trained specifically for QA\n",
        "model_name = \"deepset/roberta-base-squad2\"\n",
        "\n",
        "# loading the tokenizer + model\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
        "\n",
        "\n",
        "#creating the QA pipeline\n",
        "qa_pipeline = pipeline(\n",
        "    \"question-answering\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "print(\"Model and tokenizer loaded successfully!\")"
      ],
      "metadata": {
        "id": "s2j8bsegW5vK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chunking the Text"
      ],
      "metadata": {
        "id": "PBlS-7Goi0Ck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def chunk_text(text, max_tokens=350):\n",
        "    words = text.split()\n",
        "    chunks = []\n",
        "    current_chunk = []\n",
        "    current_length = 0\n",
        "\n",
        "    for word in words:\n",
        "        token_length = len(tokenizer.tokenize(word))\n",
        "        current_chunk.append(word)\n",
        "        current_length += token_length\n",
        "\n",
        "        if current_length >= max_tokens:\n",
        "            chunks.append(\" \".join(current_chunk))\n",
        "            current_chunk = []\n",
        "            current_length = 0\n",
        "\n",
        "    if current_chunk:\n",
        "        chunks.append(\" \".join(current_chunk))\n",
        "\n",
        "    return chunks\n",
        "\n",
        "# create the actual chunks from your clean_text\n",
        "chunks = chunk_text(book_text)\n",
        "\n",
        "len(chunks)\n"
      ],
      "metadata": {
        "id": "97b7jTgQi54R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# adding overlap to support the chunk_text function\n",
        "def chunk_text_with_overlap(text, max_tokens=200, overlap=50):\n",
        "    words = text.split()\n",
        "    chunks = []\n",
        "    current_pos = 0\n",
        "\n",
        "    while current_pos < len(words):\n",
        "        # get the slice for this chunk\n",
        "        end_pos = current_pos + max_tokens\n",
        "        chunk_words = words[current_pos:end_pos]\n",
        "\n",
        "        # append the chunk\n",
        "        chunks.append(\" \".join(chunk_words))\n",
        "\n",
        "        # move forward by max_tokens - overlap\n",
        "        # so each new chunk overlaps with previous\n",
        "        current_pos += (max_tokens - overlap)\n",
        "\n",
        "    return chunks"
      ],
      "metadata": {
        "id": "_UcCwhdJ8izs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Connecting the QA Pipeline to the Book Chunks"
      ],
      "metadata": {
        "id": "vRg5Kd4Hf9RT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# finds the top 10 chunks that shsare the most meaningful words with the question\n",
        "def retrieve_relevant_chunks(question, chunks, top_k=10):\n",
        "    question_words = set(question.lower().split())\n",
        "    ranked = []\n",
        "\n",
        "    for chunk in chunks:\n",
        "        chunk_words = set(chunk.lower().split())\n",
        "        overlap = len(question_words & chunk_words)\n",
        "        ranked.append((overlap, chunk))\n",
        "\n",
        "    ranked.sort(reverse=True, key=lambda x: x[0])\n",
        "    return [c for _, c in ranked[:top_k]]\n"
      ],
      "metadata": {
        "id": "ysFAVR-Vu0Ur"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creating the answer_question function (the brain of the QA system)\n",
        "# splits the book into chunks, searched those chunks, and finds the best answer\n",
        "\n",
        "def answer_question(question, chunks):\n",
        "    best_score = 0\n",
        "    best_answer = None\n",
        "\n",
        "    for chunk in chunks:\n",
        "        try:\n",
        "            result = qa_pipeline({\n",
        "                \"context\": chunk,\n",
        "                \"question\": question\n",
        "            })\n",
        "\n",
        "            if result[\"answer\"].strip() and result[\"score\"] > best_score:\n",
        "                best_score = result[\"score\"]\n",
        "                best_answer = result[\"answer\"]\n",
        "\n",
        "        except Exception as e:\n",
        "            pass  # ignore any chunk errors\n",
        "\n",
        "    return best_answer, best_score"
      ],
      "metadata": {
        "id": "U8Y3wtNLf06y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# testing it by ansking a real question from the book\n",
        "question = \"Where was Alice sitting at the beginning of the story?\"\n",
        "answer, score = answer_question(question, chunks)\n",
        "\n",
        "answer, score"
      ],
      "metadata": {
        "id": "PEXrOhz4gfXm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"Where was Alice sitting at the beginning of the story?\"\n",
        "\n",
        "# retrieving relevant chunks\n",
        "relevant_chunks = retrieve_relevant_chunks(question, chunks, top_k=10)\n",
        "\n",
        "# running QA on only the relevant chunks\n",
        "answer, score = answer_question(question, relevant_chunks)\n",
        "\n",
        "answer, score\n"
      ],
      "metadata": {
        "id": "LQ7SG_OauDCF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chunk_sizes = [200, 300, 400]\n",
        "results_chunk = {}\n",
        "\n",
        "for size in chunk_sizes:\n",
        "    # resplitting the entire book using the specified chunk size\n",
        "    test_chunks = chunk_text(book_text, max_tokens=size)\n",
        "\n",
        "    # retrieving the top 10 most relevant chunks for the question\n",
        "    relevant = retrieve_relevant_chunks(\n",
        "        \"Where was Alice sitting at the beginning of the story?\",\n",
        "        test_chunks,\n",
        "        top_k=10\n",
        "    )\n",
        "\n",
        "    # running the QA pipeline only on those retrieved chunks\n",
        "    ans, sc = answer_question(\n",
        "        \"Where was Alice sitting at the beginning of the story?\",\n",
        "        relevant\n",
        "    )\n",
        "    results_chunk[size] = (ans, sc, len(test_chunks))\n",
        "\n",
        "results_chunk\n",
        "\n",
        "df = pd.DataFrame([\n",
        "    {\"chunk_size\": size,\n",
        "     \"num_chunks\": results_chunk[size][2],\n",
        "     \"answer\": results_chunk[size][0],\n",
        "     \"score\": results_chunk[size][1]\n",
        "     }\n",
        "    for size in results_chunk\n",
        "])\n",
        "\n",
        "df"
      ],
      "metadata": {
        "id": "swCtV0Gku9K1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chunks_200 = chunk_text(book_text, max_tokens=200)\n",
        "topk_values = [3, 5, 10, 15]   # number of retrieved chunks to evaluate\n",
        "results_topk = {}\n",
        "\n",
        "for k in topk_values:\n",
        "\n",
        "    # retrieving top-k relevant chunks\n",
        "    relevant = retrieve_relevant_chunks(\n",
        "        \"Where was Alice sitting at the beginning of the story?\",\n",
        "        chunks_200,        # using the best chunk size you found (200)\n",
        "        top_k=k\n",
        "    )\n",
        "\n",
        "    # running QA on the retrieved chunks\n",
        "    ans, sc = answer_question(\n",
        "        \"Where was Alice sitting at the beginning of the story?\",\n",
        "        relevant\n",
        "    )\n",
        "\n",
        "    results_topk[k] = (ans, sc)\n",
        "\n",
        "results_topk\n",
        "\n",
        "df_topk = pd.DataFrame([\n",
        "    {\"top_k\": k,\n",
        "     \"answer\": results_topk[k][0],\n",
        "     \"score\": results_topk[k][1]}\n",
        "    for k in results_topk\n",
        "])\n",
        "\n",
        "df_topk\n"
      ],
      "metadata": {
        "id": "SbSFWQZH1oCo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tuning the chunk overlap\n",
        "overlap_values = [0, 25, 50, 75]\n",
        "results_overlap = {}\n",
        "\n",
        "for ov in overlap_values:\n",
        "    # creating overlapping chunks\n",
        "    overlap_chunks = chunk_text_with_overlap(book_text, max_tokens=200, overlap=ov)\n",
        "\n",
        "    # retrieving relevant chunks\n",
        "    relevant = retrieve_relevant_chunks(\n",
        "        \"Where was Alice sitting at the beginning of the story?\",\n",
        "        overlap_chunks,\n",
        "        top_k=10\n",
        "    )\n",
        "\n",
        "    # running QA\n",
        "    ans, sc = answer_question(\n",
        "        \"Where was Alice sitting at the beginning of the story?\",\n",
        "        relevant\n",
        "    )\n",
        "\n",
        "    results_overlap[ov] = (ans, sc, len(overlap_chunks))\n",
        "\n",
        "results_overlap\n",
        "\n",
        "df_overlap = pd.DataFrame([\n",
        "    {\n",
        "        \"overlap\": ov,\n",
        "        \"num_chunks\": results_overlap[ov][2],\n",
        "        \"answer\": results_overlap[ov][0],\n",
        "        \"score\": results_overlap[ov][1]\n",
        "    }\n",
        "    for ov in results_overlap\n",
        "])\n",
        "\n",
        "df_overlap"
      ],
      "metadata": {
        "id": "441ydDw_6inO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Final Chunk Set"
      ],
      "metadata": {
        "id": "Vp7uk1YqBDF5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# final chunking using best hyperparameters\n",
        "final_chunks = chunk_text_with_overlap(book_text, max_tokens=200, overlap=25)\n",
        "print(\"Total chunks:\", len(final_chunks))\n"
      ],
      "metadata": {
        "id": "-UOCSVR--XoF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def final_answer(question):\n",
        "    # retrieving top 5 relevant chunks\n",
        "    relevant = retrieve_relevant_chunks(\n",
        "        question,\n",
        "        final_chunks,\n",
        "        top_k=5\n",
        "    )\n",
        "\n",
        "    # running QA on these chunks\n",
        "    ans, score = answer_question(question, relevant)\n",
        "\n",
        "    return ans, score\n"
      ],
      "metadata": {
        "id": "_5CIK244BKFY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluation_questions = [\n",
        "    \"Who did Alice follow into the rabbit hole?\",\n",
        "    \"What was the White Rabbit looking at when Alice first saw him?\",\n",
        "    \"Who are you?\",\n",
        "    \"Who stole the tarts?\",\n",
        "    \"Who is the Queen in the Queen of Hearts scene?\"\n",
        "]\n"
      ],
      "metadata": {
        "id": "3OAeNVsbBPZJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = []\n",
        "\n",
        "for q in evaluation_questions:\n",
        "    ans, score = final_answer(q)\n",
        "    results.append((q, ans, score))\n",
        "\n",
        "results\n",
        "\n",
        "df_results = pd.DataFrame([\n",
        "    {\"Question\": q, \"Answer Returned\": a, \"Confidence Score\": round(s, 3)}\n",
        "    for q, a, s in results\n",
        "])\n",
        "\n",
        "df_results"
      ],
      "metadata": {
        "id": "QjgdtbEDBTtB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6YSehuoqBWmO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}